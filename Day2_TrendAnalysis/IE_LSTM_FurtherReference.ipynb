{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81ab9b4",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ML-&amp;-Time-series---Neural-net-models-applied-to-forecasting\" data-toc-modified-id=\"ML-&amp;-Time-series---Neural-net-models-applied-to-forecasting-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ML &amp; Time series - Neural net models applied to forecasting</a></span></li><li><span><a href=\"#1---Long-Short-Term-Memory-networks-(LSTM)-in-Python\" data-toc-modified-id=\"1---Long-Short-Term-Memory-networks-(LSTM)-in-Python-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>1 - Long Short-Term Memory networks (LSTM) in Python</a></span><ul class=\"toc-item\"><li><span><a href=\"#A-long-history-of-application-of-A.-I.-and-Time-Series\" data-toc-modified-id=\"A-long-history-of-application-of-A.-I.-and-Time-Series-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>A long history of application of A. I. and Time Series</a></span></li><li><span><a href=\"#Recurrent-Neural-Networks-(RNN)\" data-toc-modified-id=\"Recurrent-Neural-Networks-(RNN)-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Recurrent Neural Networks (RNN)</a></span></li></ul></li><li><span><a href=\"#2-Univariate-LSTM-Models---turning-a-series-into-a-muti-lines-with-X-and-y\" data-toc-modified-id=\"2-Univariate-LSTM-Models---turning-a-series-into-a-muti-lines-with-X-and-y-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>2 Univariate LSTM Models - turning a series into a muti-lines with X and y</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Vanilla-LSTM\" data-toc-modified-id=\"2.1-Vanilla-LSTM-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>2.1 Vanilla LSTM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Parameters\" data-toc-modified-id=\"Parameters-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Parameters</a></span></li><li><span><a href=\"#2.1.1--Activation-functions\" data-toc-modified-id=\"2.1.1--Activation-functions-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>2.1.1  Activation functions</a></span></li><li><span><a href=\"#2.1.2-Vanilla-LSTM---the-whole-code-so-far\" data-toc-modified-id=\"2.1.2-Vanilla-LSTM---the-whole-code-so-far-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>2.1.2 Vanilla LSTM - the whole code so far</a></span></li></ul></li></ul></li><li><span><a href=\"#2.1-Stacked-LSTM\" data-toc-modified-id=\"2.1-Stacked-LSTM-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>2.1 Stacked LSTM</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.3---Bidirectional-LSTM\" data-toc-modified-id=\"2.3---Bidirectional-LSTM-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>2.3 - Bidirectional LSTM</a></span></li><li><span><a href=\"#ConvLSTM---Convolutional-LSTM--\" data-toc-modified-id=\"ConvLSTM---Convolutional-LSTM---4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>ConvLSTM - Convolutional LSTM -</a></span></li><li><span><a href=\"#3---Multivariate-LSTM-Models---2-arrays\" data-toc-modified-id=\"3---Multivariate-LSTM-Models---2-arrays-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>3 - Multivariate LSTM Models - 2 arrays</a></span></li></ul></li><li><span><a href=\"#4---Multivariate-LSTM-Models---From-CSV\" data-toc-modified-id=\"4---Multivariate-LSTM-Models---From-CSV-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>4 - Multivariate LSTM Models - From CSV</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1---Lagging-the-data---so-that-we-can-use-time-series-data-in-a-supervised-algorithm-like-LSTM\" data-toc-modified-id=\"4.1---Lagging-the-data---so-that-we-can-use-time-series-data-in-a-supervised-algorithm-like-LSTM-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>4.1 - Lagging the data - so that we can use time series data in a supervised algorithm like LSTM</a></span></li><li><span><a href=\"#4.2---Train-&amp;-Test-Split\" data-toc-modified-id=\"4.2---Train-&amp;-Test-Split-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>4.2 - Train &amp; Test Split</a></span></li><li><span><a href=\"#4.2---Define-and-Fit-Model\" data-toc-modified-id=\"4.2---Define-and-Fit-Model-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>4.2 - Define and Fit Model</a></span></li><li><span><a href=\"#4.3---Evaluate-Model\" data-toc-modified-id=\"4.3---Evaluate-Model-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>4.3 - Evaluate Model</a></span></li></ul></li><li><span><a href=\"#Exercise:-Using-LSTM-and-all-metals-and-brent-time-series,-try-to-predict-tomorrow's-price-of-brent!\" data-toc-modified-id=\"Exercise:-Using-LSTM-and-all-metals-and-brent-time-series,-try-to-predict-tomorrow's-price-of-brent!-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exercise: Using LSTM and all metals and brent time series, try to predict tomorrow's price of brent!</a></span></li><li><span><a href=\"#References:\" data-toc-modified-id=\"References:-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>References:</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb45610",
   "metadata": {},
   "source": [
    "# ML & Time series - Neural net models applied to forecasting\n",
    "\n",
    "# 1 - Long Short-Term Memory networks (LSTM) in Python\n",
    "       \n",
    "<img src=\"img/profile_manoelgadi.png\" width=100 height=100 align=\"right\">\n",
    "\n",
    "Author: Prof. Manoel Gadi\n",
    "\n",
    "Contact: mfalonso@faculty.ie.edu\n",
    "\n",
    "Teaching Web: http://mfalonso.pythonanywhere.com\n",
    "\n",
    "Last revision: 26/June/2022\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c7abe0",
   "metadata": {},
   "source": [
    "## A long history of application of A. I. and Time Series\n",
    "However the inter-relationship between AI and Time Series are not new. In the last 30, it has been created algorithms using A. I. for time series forecasting in an isolate or in a combined way with classical statistical time-series algorithms.\n",
    "Currently the most known algoritm is the LSTM (Long short term time memory)\n",
    "review: https://neptune.ai/blog/arima-vs-prophet-vs-lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e758a",
   "metadata": {},
   "source": [
    "Long Short-Term Memory networks, or LSTMs for short, can be applied to time series forecasting.\n",
    "\n",
    "There are many types of LSTM models that can be used for each specific type of time series forecasting problem.\n",
    "\n",
    "In this tutorial, you will discover how to develop a suite of LSTM models for a range of standard time series forecasting problems.\n",
    "\n",
    "But let's start with RNNs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add06eac",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "\n",
    "__Humans don’t start their thinking from scratch every second__. As you read this essay, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. __Your thoughts have persistence__.\n",
    "\n",
    "__Traditional neural networks can’t do this__, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.\n",
    "\n",
    "__RNNs address this issue by introduccing a loops, allowing information to persist__.\n",
    "\n",
    "\n",
    "<img src=\"img/RNN.png\" width=500 height=500 align=\"center\">\n",
    "\n",
    "However, __RNN suffer from the long term dependency problem__. Overtime, with more information, RNN become less effective of learning new things.\n",
    "\n",
    "\n",
    "LSTM provides solution to long term dependency problem, by introduccing a cell state.\n",
    "<img src=\"img/LSTM_Cell2.png\" width=500 height=500 align=\"center\">\n",
    "\n",
    "The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Let's watch this video and stop it to compare with ARIMA models, then maybe things become a bit clearer:\n",
    "\n",
    "<a href=\"https://www.youtube.com/watch?v=b61DPVFX03I\" target=\"_blank\">\n",
    "<img src=\"img/LSTM_intro.png\" width=600 height=600 align=\"center\"> </a>\n",
    "\n",
    "---\n",
    "\n",
    "If you want to go deeper in the math behind, you can have a look in to:\n",
    "https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "In the rest of this Notebook we will see:\n",
    "\n",
    "* How to develop LSTM models for univariate time series forecasting.\n",
    "* How to develop LSTM models for multivariate time series forecasting.\n",
    "* How to develop LSTM models for multi-step time series forecasting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a3f67",
   "metadata": {},
   "source": [
    "# 2 Univariate LSTM Models - turning a series into a muti-lines with X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d890d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:41:19.171914Z",
     "start_time": "2022-07-06T12:41:19.167914Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "raw_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f2eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:41:16.730918Z",
     "start_time": "2022-07-06T12:41:16.724916Z"
    }
   },
   "outputs": [],
   "source": [
    "# univariate data preparation\n",
    "from numpy import array\n",
    " \n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7e24a",
   "metadata": {},
   "source": [
    "The idea here is that __every row__ being predicted should cointain the __required lagged data__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a8fe43",
   "metadata": {},
   "source": [
    "## 2.1 Vanilla LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f11b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:41:43.487790Z",
     "start_time": "2022-07-06T12:41:43.484787Z"
    }
   },
   "outputs": [],
   "source": [
    "n_steps \n",
    "n_features = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4b810",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:47:35.349569Z",
     "start_time": "2022-07-06T12:47:35.346569Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d81a726",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:47:36.213580Z",
     "start_time": "2022-07-06T12:47:36.211580Z"
    }
   },
   "outputs": [],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15d8ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:47:43.021530Z",
     "start_time": "2022-07-06T12:47:37.893548Z"
    }
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "model = Sequential()\n",
    "\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d40e3",
   "metadata": {},
   "source": [
    "### Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef1a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:47:46.537062Z",
     "start_time": "2022-07-06T12:47:46.533062Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412351f",
   "metadata": {},
   "source": [
    "* 50 = units: Positive integer, dimensionality of the output space\n",
    "* activation: Activation function to use. Default: hyperbolic tangent (tanh). If you pass None, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "* https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\n",
    "\n",
    "By default, activation=tanh\n",
    "<img src=\"img/tanh.png\" width=400 height=400 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf99ea",
   "metadata": {},
   "source": [
    "### 2.1.1  Activation functions\n",
    "\n",
    "Activation Functions: Sigmoid, Tanh, ReLU, Leaky ReLU, Softmax\n",
    "\n",
    "Generally , we use Rectified Linear Unit (__ReLU__) in hidden layer to avoid vanishing gradient problem and better computation performance , and Softmax function use in last output layer.\n",
    "\n",
    "ReLU (Rectified Linear Unit):\n",
    "__ReLU is most popular activation function which is used in hidden layer of NN__.The formula is deceptively simple: 𝑚𝑎𝑥(0,𝑧). Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid but with better performance.\n",
    "\n",
    "\n",
    "<img src=\"img/ReLU.png\" width=400 height=400 align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa131bc",
   "metadata": {},
   "source": [
    "n_features = 1 as we have a univariate series [10, 20, 30, 40, 50, 60, 70, 80, 90]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cb134d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:48:31.621473Z",
     "start_time": "2022-07-06T12:48:31.618470Z"
    }
   },
   "outputs": [],
   "source": [
    "X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e9182",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:48:49.119296Z",
     "start_time": "2022-07-06T12:48:47.711240Z"
    }
   },
   "outputs": [],
   "source": [
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a880ea54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:50:40.886891Z",
     "start_time": "2022-07-06T12:50:40.843918Z"
    }
   },
   "outputs": [],
   "source": [
    "# predicting for a new data: [70, 80, 90]\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887961f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e4e61ad",
   "metadata": {},
   "source": [
    "### 2.1.2 Vanilla LSTM - the whole code so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c15cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:50:16.383565Z",
     "start_time": "2022-07-06T12:50:14.863564Z"
    }
   },
   "outputs": [],
   "source": [
    "# univariate lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    " \n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0706c0",
   "metadata": {},
   "source": [
    "# 2.1 Stacked LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d0c977",
   "metadata": {},
   "source": [
    "Multiple hidden LSTM layers can be stacked one on top of another in what is referred to as a Stacked LSTM model.\n",
    "\n",
    "An LSTM layer requires a three-dimensional input and LSTMs by default will produce a two-dimensional output as an interpretation from the end of the sequence.\n",
    "\n",
    "We can address this by having the LSTM output a value for each time step in the input data by setting the return_sequences=True argument on the layer. This allows us to have 3D output from hidden LSTM layer as input to the next.\n",
    "\n",
    "We can therefore define a Stacked LSTM as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f899a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:52:56.474613Z",
     "start_time": "2022-07-06T12:52:53.227447Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    " \n",
    "# split a univariate sequence\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8308589d",
   "metadata": {},
   "source": [
    "## 2.3 - Bidirectional LSTM\n",
    "On some sequence prediction problems, it can be beneficial to allow the LSTM model to learn the input sequence both forward and backwards and concatenate both interpretations.\n",
    "\n",
    "This is called a Bidirectional LSTM.\n",
    "\n",
    "We can implement a Bidirectional LSTM for univariate time series forecasting by wrapping the first hidden layer in a wrapper layer called Bidirectional.\n",
    "\n",
    "An example of defining a Bidirectional LSTM to read input both forward and backward is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444a37de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:55:01.533494Z",
     "start_time": "2022-07-06T12:54:58.952493Z"
    }
   },
   "outputs": [],
   "source": [
    "# univariate bidirectional lstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Bidirectional\n",
    " \n",
    "# split a univariate sequence\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe4585",
   "metadata": {},
   "source": [
    "## ConvLSTM - Convolutional LSTM - \n",
    "\n",
    "A convolutional neural network, or CNN for short, is a type of neural network developed for working with two-dimensional image data.\n",
    "\n",
    "The CNN can be very effective at automatically extracting and learning features.\n",
    "\n",
    "A type of LSTM related to the CNN-LSTM is the ConvLSTM, where the convolutional reading of input is built directly into each LSTM unit.\n",
    "\n",
    "The ConvLSTM was developed for reading two-dimensional spatial-temporal data, but can be adapted for use with univariate time series forecasting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffb21b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T12:57:48.048013Z",
     "start_time": "2022-07-06T12:57:43.104002Z"
    }
   },
   "outputs": [],
   "source": [
    "# univariate convlstm example\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import ConvLSTM2D\n",
    " \n",
    "# split a univariate sequence into samples\n",
    "def split_sequence(sequence, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequence)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the sequence\n",
    "        if end_ix > len(sequence)-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "# choose a number of time steps\n",
    "n_steps = 4\n",
    "# split into samples\n",
    "X, y = split_sequence(raw_seq, n_steps)\n",
    "# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]\n",
    "n_features = 1\n",
    "n_seq = 2\n",
    "n_steps = 2\n",
    "X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=500, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([60, 70, 80, 90])\n",
    "x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69139cd1",
   "metadata": {},
   "source": [
    "## 3 - Multivariate LSTM Models - 2 arrays\n",
    "A problem may have two or more parallel input time series and an output time series that is dependent on the input time series.\n",
    "\n",
    "The input time series are parallel because each series has an observation at the same time steps.\n",
    "\n",
    "We can demonstrate this with a simple example of two parallel input time series where the output series is the simple addition of the input series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acf368f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:00:17.108093Z",
     "start_time": "2022-07-06T13:00:17.103087Z"
    }
   },
   "outputs": [],
   "source": [
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce0716",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:00:40.110643Z",
     "start_time": "2022-07-06T13:00:40.104641Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "from numpy import hstack\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd2eaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:02:20.501661Z",
     "start_time": "2022-07-06T13:02:20.490659Z"
    }
   },
   "outputs": [],
   "source": [
    "# multivariate data preparation\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "print(X.shape, y.shape)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "    print(X[i], y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf920a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:04:44.552481Z",
     "start_time": "2022-07-06T13:04:42.918479Z"
    }
   },
   "outputs": [],
   "source": [
    "# multivariate lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    " \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "        # check if we are beyond the dataset\n",
    "        if end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix-1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps = 3\n",
    "# convert into input/output\n",
    "X, y = split_sequences(dataset, n_steps)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([[80, 85], [90, 95], [100, 105]])\n",
    "x_input = x_input.reshape((1, n_steps, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d641e8a",
   "metadata": {},
   "source": [
    "# 4 - Multivariate LSTM Models - From CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3787fdfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:06:27.265067Z",
     "start_time": "2022-07-06T13:06:25.794063Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from datetime import datetime\n",
    "# load data\n",
    "def parse(x):\n",
    "    return datetime.strptime(x, '%Y %m %d %H')\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/jbrownlee/Datasets/master/pollution.csv',  parse_dates = [['year', 'month', 'day', 'hour']], index_col=0, date_parser=parse)\n",
    "dataset.drop('No', axis=1, inplace=True)\n",
    "# manually specify column names\n",
    "dataset.columns = ['pollution', 'dew', 'temp', 'press', 'wnd_dir', 'wnd_spd', 'snow', 'rain']\n",
    "dataset.index.name = 'date'\n",
    "# mark all NA values with 0\n",
    "dataset['pollution'].fillna(0, inplace=True)\n",
    "# drop the first 24 hours\n",
    "dataset = dataset[24:]\n",
    "# summarize first 5 rows\n",
    "print(dataset.head(5))\n",
    "# save to file\n",
    "dataset.to_csv('data/pollution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a898c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:07:06.679233Z",
     "start_time": "2022-07-06T13:07:05.689235Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "# load dataset\n",
    "dataset = pd.read_csv('data/pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "# specify columns to plot\n",
    "groups = [0, 1, 2, 3, 5, 6, 7]\n",
    "i = 1\n",
    "# plot each column\n",
    "pyplot.figure()\n",
    "for group in groups:\n",
    "    pyplot.subplot(len(groups), 1, i)\n",
    "    pyplot.plot(values[:, group])\n",
    "    pyplot.title(dataset.columns[group], y=0.5, loc='right')\n",
    "    i += 1\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b81ca",
   "metadata": {},
   "source": [
    "## 4.1 - Lagging the data - so that we can use time series data in a supervised algorithm like LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476719d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:19:10.985935Z",
     "start_time": "2022-07-06T13:19:10.870934Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    " \n",
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " \n",
    "# load dataset\n",
    "dataset = pd.read_csv('data/pollution.csv', header=0, index_col=0)\n",
    "values = dataset.values\n",
    "# integer encode direction\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4])\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "reframed.drop(reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bea875",
   "metadata": {},
   "source": [
    "## 4.2 - Train & Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b08461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:19:13.157489Z",
     "start_time": "2022-07-06T13:19:13.153487Z"
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = 365 * 24\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111ccdd",
   "metadata": {},
   "source": [
    "## 4.2 - Define and Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a80608",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:20:06.466113Z",
     "start_time": "2022-07-06T13:19:23.113709Z"
    }
   },
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "#model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "\n",
    "#model.add(LSTM(50))\n",
    "\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70013b3",
   "metadata": {},
   "source": [
    "## 4.3 - Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaad98a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:20:15.709146Z",
     "start_time": "2022-07-06T13:20:13.633113Z"
    }
   },
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 1:]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 1:]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd3039",
   "metadata": {},
   "source": [
    "# Exercise: Using LSTM and all metals and brent time series, try to predict tomorrow's price of brent!\n",
    "\n",
    "\n",
    "Gold (GC=F)\n",
    "Silver (SI=F)\n",
    "Platinum (PL=F)\n",
    "Palladium (PA=F)\n",
    "Natural Gas (NG=F)\n",
    "Copper (HG=F)\n",
    "Corn (ZC=F)\n",
    "Soybean Oil (ZL=F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d54e3-5fdb-448e-8442-a07d7e3c5575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from pandas_datareader import data as pdr\n",
    "import yfinance as yfin\n",
    "yfin.pdr_override()\n",
    "\n",
    "import datetime\n",
    "stock_symbol='BZ=F'\n",
    "start=datetime.datetime(2024,3,1)\n",
    "\n",
    "df_brent = pdr.get_data_yahoo(stock_symbol, start=start)\n",
    "df_brent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a14d81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:37:04.732110Z",
     "start_time": "2022-07-06T13:37:04.728107Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df_brent[['Adj Close']]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7555f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:37:05.346512Z",
     "start_time": "2022-07-06T13:37:05.339511Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe751b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22801e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:37:05.573508Z",
     "start_time": "2022-07-06T13:37:05.555510Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalize features\n",
    "#scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#scaled = scaler.fit_transform(values)\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(df, 1, 1)\n",
    "# drop columns we don't want to predict\n",
    "#reframed.drop(reframed.columns[[9,10,11,12,13,14,15,16,17,18]], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db82fbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7373ec1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:37:06.250512Z",
     "start_time": "2022-07-06T13:37:06.244509Z"
    }
   },
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "values = reframed.values\n",
    "n_train_hours = int(reframed.shape[0]*0.8) #80% for training\n",
    "train = values[:n_train_hours, :]\n",
    "test = values[n_train_hours:, :]\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, 0:9], train[:, -1]\n",
    "test_X, test_y = test[:, 0:9], test[:, -1]\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
    "test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98271af5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:37:11.044512Z",
     "start_time": "2022-07-06T13:37:06.748507Z"
    }
   },
   "outputs": [],
   "source": [
    "# design network\n",
    "model = Sequential()\n",
    "#model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "\n",
    "#model.add(LSTM(50))\n",
    "\n",
    "model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ad3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], test_X.shape[2]))\n",
    "\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, 0:9]), axis=1)\n",
    "#inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, 0:9]), axis=1)\n",
    "#inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "\n",
    "# calculate RMSE\n",
    "rmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd88e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a line plot\n",
    "plt.plot(inv_y, label='Actual')\n",
    "plt.plot(inv_yhat, label='Predicted')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Target Variable')\n",
    "plt.title('Actual vs. Predicted Values')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd59cf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_brent.to_csv(\"data/brent.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf16797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa6963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a161ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24c3eafa",
   "metadata": {},
   "source": [
    "# References:\n",
    "Understanding LSTM Networks - https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "How to Develop LSTM Models for Time Series Forecasting:\n",
    "https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/\n",
    "\n",
    "Multivariate Time Series Forecasting with LSTMs in Keras - https://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/\n",
    "\n",
    "Hyper Parameter Tunning:\n",
    "https://machinelearningmastery.com/tune-lstm-hyperparameters-keras-time-series-forecasting/\n",
    "\n",
    "More on data prep for LSTMs here:\n",
    "https://machinelearningmastery.com/faq/single-faq/how-do-i-prepare-my-data-for-an-lstm\n",
    "\n",
    "You can use diagnostics to see if the model is well suited:\n",
    "https://machinelearningmastery.com/diagnose-overfitting-underfitting-lstm-models/\n",
    "\n",
    "You can use grid/random searches of hyperparameters to see if you can do better.\n",
    "\n",
    "Error is relative, and “good” performance is determined against a baseline method like persistence, more here:\n",
    "https://machinelearningmastery.com/how-to-know-if-your-machine-learning-model-has-good-performance/\n",
    "\n",
    "More on how to change a neural net to/form regression to classification:\n",
    "https://machinelearningmastery.com/faq/single-faq/how-can-i-change-a-neural-network-from-regression-to-classification\n",
    "\n",
    "optimal lag selection multivariate time series LSTM:\n",
    "https://www.google.com/search?q=optimal+lag+selection+multivariate+time+series+LSTM&rlz=1C1GCEA_enES852ES852&oq=optimal+lag+selection+multivariate+time+series+LSTM&aqs=chrome..69i57.26984j0j7&sourceid=chrome&ie=UTF-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af40e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
